{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = np.loadtxt(\"datasets/mnist_small_train_in.txt\", delimiter=',', usecols=range(784))\n",
    "y_train = np.loadtxt(\"datasets/mnist_small_train_out.txt\")\n",
    "X_test = np.loadtxt(\"datasets/mnist_small_test_in.txt\", delimiter=',', usecols=range(784))\n",
    "y_test = np.loadtxt(\"datasets/mnist_small_test_out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for labels\n",
    "def one_hot_encoding(y_train):\n",
    "    n = y_train.shape[0]\n",
    "    y_onehot = np.zeros((n, 10))\n",
    "    for i in range(n):\n",
    "        number = int(y_train[i])\n",
    "        y_onehot[i][number] = 1\n",
    "    return y_onehot.T\n",
    "\n",
    "# activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "# for classification at the output layer\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    \n",
    "\n",
    "# categorical cross entropy loss function\n",
    "def compute_loss(Y, pred):\n",
    "    return -np.sum(Y * np.log(pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15259.964317734743\n",
      "500 788.5058954680916\n",
      "1000 215.4832272953175\n",
      "1500 81.88800600951497\n",
      "2000 42.861859321574485\n",
      "2500 27.906208832578013\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2.\n",
    "h1 = 500 # number of neurons in the first hidden layer\n",
    "h2 = 25 # number of neurons in the second hidden layer\n",
    "n = X_train.shape[1] # number of features, i.e. number of neurons in the input layer\n",
    "d = 10 # number of digits, i.e. number of neurons in the output layer\n",
    "m = X_train.shape[0] # number of training examples\n",
    "\n",
    "batch_size = 128\n",
    "batches = m // batch_size\n",
    "\n",
    "\n",
    "# Initialize weight and bias parameters for first and second layer\n",
    "w1 = np.random.randn(h1, n) * np.sqrt(1. / n) # divide by the variance for better initialization\n",
    "b1 = np.zeros((h1, 1)) * np.sqrt(1. / n)\n",
    "w2 = np.random.randn(h2, h1) * np.sqrt(1. / h1)\n",
    "b2 = np.zeros((h2, 1)) * np.sqrt(1. / h1)\n",
    "w3 = np.random.randn(d, h2) * np.sqrt(1. / h2)\n",
    "b3 = np.zeros((d, 1)) * np.sqrt(1. / h2)\n",
    "\n",
    "X = X_train.T\n",
    "Y = one_hot_encoding(y_train)\n",
    "\n",
    "# for gradient descent with momentum\n",
    "b = .9\n",
    "dw3_mom = np.zeros(w3.shape)\n",
    "db3_mom = np.zeros(b3.shape)\n",
    "dw2_mom = np.zeros(w2.shape)\n",
    "db2_mom = np.zeros(b2.shape)\n",
    "dw1_mom = np.zeros(w1.shape)\n",
    "db1_mom = np.zeros(b1.shape)\n",
    "\n",
    "# train\n",
    "for i in range(3000): # number of epochs\n",
    "    \n",
    "    permutation = np.random.permutation(X.shape[1])\n",
    "    X = X[:, permutation]\n",
    "    Y = Y[:, permutation]\n",
    "    \n",
    "    \n",
    "    # forward pass\n",
    "    z1 = w1@X + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = w2@a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = w3@a2 + b3\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    loss = compute_loss(Y, a3)\n",
    "    \n",
    "    # backward pass\n",
    "    dz3 = a3 - Y\n",
    "    dw3 = (dz3@a2.T) / m\n",
    "    db3 = dz3@np.ones((m, 1)) / m \n",
    "    \n",
    "    da2 = w3.T@dz3\n",
    "    dz2 = da2 * sigmoid(z2) * (1 - sigmoid(z2)) \n",
    "    dw2 = dz2@a1.T / m\n",
    "    db2 = dz2@np.ones((m ,1)) / m  \n",
    "    \n",
    "    da1 = w2.T@dz2\n",
    "    dz1 = da1 * sigmoid(z1) * (1 - sigmoid(z1)) \n",
    "    dw1 = dz1@X.T / m\n",
    "    db1 = dz1@np.ones((m ,1)) / m  \n",
    "    \n",
    "    # optimization of parameters\n",
    "    dw3_mom = (b * dw3_mom) + (1 - b) * dw3\n",
    "    db3_mom = (b * db3_mom) + (1 - b) * db3\n",
    "    dw2_mom = (b * dw2_mom) + (1 - b) * dw2\n",
    "    db2_mom = (b * db2_mom) + (1 - b) * db2\n",
    "    dw1_mom = (b * dw1_mom) + (1 - b) * dw1\n",
    "    db1_mom = (b * db1_mom) + (1 - b) * db1\n",
    "    \n",
    "    w3 = w3 - learning_rate * dw3_mom\n",
    "    b3 = b3 - learning_rate * db3_mom\n",
    "    w2 = w2 - learning_rate * dw2_mom\n",
    "    b2 = b2 - learning_rate * db2_mom\n",
    "    w1 = w1 - learning_rate * dw1_mom\n",
    "    b1 = b1 - learning_rate * db1_mom\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95        99\n",
      "           1       0.99      0.95      0.97       119\n",
      "           2       0.92      0.91      0.92       105\n",
      "           3       0.89      0.92      0.90        98\n",
      "           4       0.87      0.91      0.89        94\n",
      "           5       0.82      0.86      0.84        86\n",
      "           6       0.95      0.94      0.94        97\n",
      "           7       0.94      0.91      0.92       107\n",
      "           8       0.92      0.93      0.92        97\n",
      "           9       0.90      0.89      0.90       102\n",
      "\n",
      "    accuracy                           0.92      1004\n",
      "   macro avg       0.92      0.92      0.92      1004\n",
      "weighted avg       0.92      0.92      0.92      1004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Test on the test set\n",
    "Y_test = one_hot_encoding(y_test)\n",
    "X = X_test.T\n",
    "\n",
    "# forward pass with the test data\n",
    "z1 = w1@X + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = w2@a1 + b2\n",
    "a2 = sigmoid(z2)\n",
    "z3 = w3@a2 + b3\n",
    "a3 = softmax(z3)\n",
    "\n",
    "predictions = np.argmax(a3, axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
